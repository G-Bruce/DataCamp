{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study Ch3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries for Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip lists: zipped_lists\n",
    "zipped_lists = zip(feature_names, row_vals)\n",
    "\n",
    "# Create a dictionary: rs_dict\n",
    "rs_dict = dict(zipped_lists)\n",
    "\n",
    "# Print the dictionary\n",
    "print(rs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists2dict()\n",
    "def lists2dict(list1, list2):\n",
    "    \"\"\"Return a dictionary where list1 provides\n",
    "    the keys and list2 provides the values.\"\"\"\n",
    "\n",
    "    # Zip lists: zipped_lists\n",
    "    zipped_lists = zip(list1, list2)\n",
    "\n",
    "    # Create a dictionary: rs_dict\n",
    "    rs_dict = dict(zipped_lists)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return(rs_dict)\n",
    "\n",
    "# Call lists2dict: rs_fxn\n",
    "rs_fxn = lists2dict(feature_names, row_vals)\n",
    "\n",
    "# Print rs_fxn\n",
    "print(rs_fxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first two lists in row_lists\n",
    "print(row_lists[0])\n",
    "print(row_lists[1])\n",
    "\n",
    "# OUTPUT FROM ABOVE LINE:\n",
    "# ['Arab World', 'ARB', 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'SP.ADO.TFRT', '1960', '133.56090740552298']\n",
    "# ['Arab World', 'ARB', 'Age dependency ratio (% of working-age population)', 'SP.POP.DPND', '1960', '87.7976011532547']\n",
    "\n",
    "# feature_names\n",
    "# ['CountryName', 'CountryCode', 'IndicatorName', 'IndicatorCode', 'Year', 'Value']\n",
    "\n",
    "# Turn list of lists into list of dicts: list_of_dicts\n",
    "list_of_dicts = [lists2dict(feature_names,sublist) for sublist in row_lists]\n",
    "\n",
    "# Print the first two dictionaries in list_of_dicts\n",
    "print(list_of_dicts[0])\n",
    "# {'Value': '133.56090740552298', 'IndicatorCode': 'SP.ADO.TFRT', 'CountryCode': 'ARB', 'Year': '1960', 'CountryName': 'Arab World', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)'}\n",
    "print(list_of_dicts[1])\n",
    "# {'Value': '87.7976011532547', 'IndicatorCode': 'SP.POP.DPND', 'CountryCode': 'ARB', 'Year': '1960', 'CountryName': 'Arab World', 'IndicatorName': 'Age dependency ratio (% of working-age population)'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning this all into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Turn list of lists into list of dicts: list_of_dicts\n",
    "list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]\n",
    "\n",
    "# Turn list of dicts into a DataFrame: df\n",
    "df = pd.DataFrame(list_of_dicts)\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "#  CountryCode CountryName   IndicatorCode  \\\n",
    "#    0         ARB  Arab World     SP.ADO.TFRT   \n",
    "#    1         ARB  Arab World     SP.POP.DPND   \n",
    "#    2         ARB  Arab World  SP.POP.DPND.OL   \n",
    "#    3         ARB  Arab World  SP.POP.DPND.YG   \n",
    "#    4         ARB  Arab World  MS.MIL.XPRT.KD   \n",
    "    \n",
    "#                                           IndicatorName               Value  Year  \n",
    "#    0  Adolescent fertility rate (births per 1,000 wo...  133.56090740552298  1960  \n",
    "#    1  Age dependency ratio (% of working-age populat...    87.7976011532547  1960  \n",
    "#    2  Age dependency ratio, old (% of working-age po...   6.634579191565161  1960  \n",
    "#    3  Age dependency ratio, young (% of working-age ...   81.02332950839141  1960  \n",
    "#    4        Arms exports (SIPRI trend indicator values)           3000000.0  1960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data in chunks (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, data sources can be so large in size that storing the entire dataset in memory becomes too resource-intensive. In this exercise, you will process the first 1000 rows of a file line by line, to create a dictionary of the counts of how many times each country appears in a column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the file\n",
    "with open('world_dev_ind.csv') as file:\n",
    "\n",
    "    # Skip the column names\n",
    "    file.readline()\n",
    "\n",
    "    # Initialize an empty dictionary: counts_dict\n",
    "    counts_dict = {}\n",
    "\n",
    "    # Process only the first 1000 rows\n",
    "    for j in range(1000):\n",
    "\n",
    "        # Split the current line into a list: line\n",
    "        line = file.readline().split(',')\n",
    "\n",
    "        # Get the value for the first column: first_col\n",
    "        first_col = line[0]\n",
    "\n",
    "        # If the column value is in the dict, increment its value\n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "\n",
    "        # Else, add to the dict and set value to 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(counts_dict)\n",
    "\n",
    "#    {'Europe & Central Asia (developing only)': 89, 'Euro area': 119, 'East Asia & Pacific (developing only)': 123, 'European Union': 116, 'Central Europe and the Baltics': 71, 'East Asia & Pacific (all income levels)': 122, 'Fragile and conflict affected situations': 76, 'Heavily indebted poor countries (HIPC)': 18, 'Arab World': 80, 'Europe & Central Asia (all income levels)': 109, 'Caribbean small states': 77}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a generator to load data in chunks (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, you processed a file line by line for a given number of lines. What if, however, you want to do this for the entire file?\n",
    "\n",
    "In this case, it would be useful to use generators. Generators allow users to lazily evaluate data. This concept of lazy evaluation is useful when you have to deal with very large datasets because it lets you generate values in an efficient manner by yielding only chunks of data at a time instead of the whole thing at once.\n",
    "\n",
    "In this exercise, you will define a generator function read_large_file() that produces a generator object which yields a single line from a file each time next() is called on it. The csv file 'world_dev_ind.csv' is in your current directory for your use.\n",
    "\n",
    "Note that when you open a connection to a file, the resulting file object is already a generator! So out in the wild, you won't have to explicitly create generator objects in cases such as this. However, for pedagogical reasons, we are having you practice how to do this here with the read_large_file() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define read_large_file()\n",
    "def read_large_file(file_object):\n",
    "    \"\"\"A generator function to read a large file lazily.\"\"\"\n",
    "\n",
    "    # Loop indefinitely until the end of the file\n",
    "    while True:\n",
    "\n",
    "        # Read a line from the file: data\n",
    "        data = file_object.readline()\n",
    "\n",
    "        # Break if this is the end of the file\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Yield the line of data\n",
    "        yield data\n",
    "\n",
    "# Open a connection to the file\n",
    "with open('world_dev_ind.csv') as file:\n",
    "\n",
    "    # Create a generator object for the file: gen_file\n",
    "    gen_file = read_large_file(file)\n",
    "\n",
    "    # Print the first three lines of the file\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    \n",
    "# output:\n",
    "#    CountryName,CountryCode,IndicatorName,IndicatorCode,Year,Value\n",
    "#    \n",
    "#    Arab World,ARB,\"Adolescent fertility rate (births per 1,000 women ages 15-19)\",SP.ADO.TFRT,1960,133.56090740552298\n",
    "#    \n",
    "#    Arab World,ARB,Age dependency ratio (% of working-age population),SP.POP.DPND,1960,87.7976011532547\n",
    "\n",
    "\n",
    "# NOTE: that since a file object is already a generator, you don't have to explicitly create a generator object with \n",
    "# your read_large_file() function. However, it is still good to practice how to create generators - well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a generator to load data in chunks (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You've just created a generator function that you can use to help you process large files.\n",
    "\n",
    "Now let's use your generator function to process the World Bank dataset like you did previously. You will process the file line by line, to create a dictionary of the counts of how many times each country appears in a column in the dataset. For this exercise, however, you won't process just 1000 rows of data, you'll process the entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary: counts_dict\n",
    "counts_dict = {}\n",
    "\n",
    "# Open a connection to the file\n",
    "with open('world_dev_ind.csv') as file:\n",
    "\n",
    "    # Iterate over the generator from read_large_file()\n",
    "    for line in read_large_file(file):\n",
    "\n",
    "        row = line.split(',')\n",
    "        first_col = row[0]\n",
    "\n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1\n",
    "\n",
    "# Print            \n",
    "print(counts_dict)\n",
    "\n",
    "# {'Latin America & Caribbean (developing only)': 133, 'High income: OECD': 127, 'Middle income': 138, 'Low income': 80, \n",
    "# 'Small states': 69, 'Caribbean small states': 77, 'European Union': 116, 'CountryName': 1, 'OECD members': 130, \n",
    "# 'Lower middle income': 126, 'East Asia & Pacific (developing only)': 123, 'Euro area': 119, \n",
    "# 'Europe & Central Asia (developing only)': 89, 'South Asia': 36, 'East Asia & Pacific (all income levels)': 122, \n",
    "# 'Fragile and conflict affected situations': 76, 'Other small states': 63, 'Europe & Central Asia (all income levels)': 109, \n",
    "# 'Central Europe and the Baltics': 71, 'North America': 123, 'Low & middle income': 138, 'High income': 131, \n",
    "# 'Heavily indebted poor countries (HIPC)': 99, 'Pacific island small states': 66, \n",
    "# 'Least developed countries: UN classification': 78, 'Middle East & North Africa (developing only)': 94, 'Arab World': 80, \n",
    "# 'High income: nonOECD': 68, 'Middle East & North Africa (all income levels)': 89, \n",
    "# 'Latin America & Caribbean (all income levels)': 130}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an iterator to load data in chunks (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain length, say, 100. For example, with the pandas package (imported as pd), you can do pd.read_csv(filename, chunksize=100). This creates an iterable reader object, which means that you can use next() on it.\n",
    "\n",
    "In this exercise, you will read a file in small DataFrame chunks with read_csv(). You're going to use the World Bank Indicators data 'ind_pop.csv', available in your current directory, to look at the urban population indicator for numerous countries and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize reader object: df_reader\n",
    "df_reader = pd.read_csv('ind_pop.csv', chunksize=10)\n",
    "\n",
    "# Print two chunks\n",
    "print(next(df_reader))\n",
    "print(next(df_reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an iterator to load data in chunks (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, you used read_csv() to read in DataFrame chunks from a large dataset. In this exercise, you will read in a file using a bigger DataFrame chunk size and then process the data from the first chunk.\n",
    "\n",
    "To process the data, you will create another DataFrame composed of only the rows from a specific country. You will then zip together two of the columns from the new DataFrame, 'Total Population' and 'Urban population (% of total)'. Finally, you will create a list of tuples from the zip object, where each tuple is composed of a value from each of the two columns mentioned.\n",
    "\n",
    "You're going to use the data from 'ind_pop_data.csv', available in your current directory. Pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reader object: urb_pop_reader\n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)\n",
    "\n",
    "# Get the first DataFrame chunk: df_urb_pop\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "\n",
    "# Check out the head of the DataFrame\n",
    "print(df_urb_pop.head())\n",
    "\n",
    "# Check out specific country: df_pop_ceb\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "\n",
    "# Zip DataFrame columns of interest: pops\n",
    "pops = zip(df_pop_ceb['Total Population'], \n",
    "df_pop_ceb['Urban population (% of total)'])\n",
    "\n",
    "# Turn zip object into list: pops_list\n",
    "pops_list = list(pops)\n",
    "\n",
    "# Print pops_list\n",
    "print(pops_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an iterator to load data in chunks (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're getting used to reading and processing data in chunks by now. Let's push your skills a little further by adding a column to a DataFrame.\n",
    "\n",
    "Starting from the code of the previous exercise, you will be using a list comprehension to create the values for a new column 'Total Urban Population' from the list of tuples that you generated earlier. Recall from the previous exercise that the first and second elements of each tuple consist of, respectively, values from the columns 'Total Population' and 'Urban population (% of total)'. The values in this new column 'Total Urban Population', therefore, are the product of the first and second element in each tuple. Furthermore, because the 2nd element is a percentage, you need to divide the entire result by 100, or alternatively, multiply it by 0.01.\n",
    "\n",
    "You will also plot the data from this new column to create a visualization of the urban population data.\n",
    "\n",
    "The packages pandas and matplotlib.pyplot have been imported as pd and plt respectively for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from previous exercise\n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "\n",
    "# df_urb_pop\n",
    "#                                         CountryName CountryCode  Year  \\\n",
    "# 0                                        Arab World         ARB  1960   \n",
    "# 1                            Caribbean small states         CSS  1960   \n",
    "# 2                    Central Europe and the Baltics         CEB  1960   \n",
    "# 3           East Asia & Pacific (all income levels)         EAS  1960   \n",
    "# 4             East Asia & Pacific (developing only)         EAP  1960 \n",
    "\n",
    "#      Total Population  Urban population (% of total)  \n",
    "# 0        9.249590e+07                      31.285384  \n",
    "# 1        4.190810e+06                      31.597490  \n",
    "# 2        9.140158e+07                      44.507921  \n",
    "# 3        1.042475e+09                      22.471132  \n",
    "# 4        8.964930e+08                      16.917679\n",
    "\n",
    "# ...\n",
    "# [1000 rows x 5 columns]\n",
    "\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "\n",
    "#                        CountryName CountryCode  Year  Total Population  \\\n",
    "# 2    Central Europe and the Baltics         CEB  1960        91401583.0   \n",
    "# 244  Central Europe and the Baltics         CEB  1961        92237118.0   \n",
    "# 486  Central Europe and the Baltics         CEB  1962        93014890.0   \n",
    "# 728  Central Europe and the Baltics         CEB  1963        93845749.0   \n",
    "# 970  Central Europe and the Baltics         CEB  1964        94722599.0   \n",
    "\n",
    "#      Urban population (% of total)  \n",
    "# 2                        44.507921  \n",
    "# 244                      45.206665  \n",
    "# 486                      45.866565  \n",
    "# 728                      46.534093  \n",
    "# 970                      47.208743\n",
    "\n",
    "\n",
    "pops = zip(df_pop_ceb['Total Population'], \n",
    "           df_pop_ceb['Urban population (% of total)'])\n",
    "pops_list = list(pops)\n",
    "\n",
    "# pops_list\n",
    "#[(91401583.0, 44.5079211390026),\n",
    "# (92237118.0, 45.206665319194),\n",
    "# (93014890.0, 45.866564696018),\n",
    "# (93845749.0, 46.5340927663649),\n",
    "# (94722599.0, 47.2087429803526)]\n",
    "\n",
    "# Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "# tup[0] references the first column within the \"target\" within the iterable \"pops_list\"\n",
    "# tup[1] references the second column within the \"target\" within the iterable \"pops_list\"\n",
    "df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "\n",
    "# Plot urban population data\n",
    "df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an iterator to load data in chunks (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you've only processed the data from the first DataFrame chunk. This time, you will aggregate the results over all the DataFrame chunks in the dataset. This basically means you will be processing the entire dataset now. This is neat because you're going to be able to process the entire large dataset by just working on smaller pieces of it!\n",
    "\n",
    "You're going to use the data from 'ind_pop_data.csv', available in your current directory. The packages pandas and matplotlib.pyplot have been imported as pd and plt respectively for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reader object: urb_pop_reader\n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)\n",
    "\n",
    "# Initialize empty DataFrame: data\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each DataFrame chunk\n",
    "for df_urb_pop in urb_pop_reader:\n",
    "\n",
    "    # Check out specific country: df_pop_ceb\n",
    "    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "\n",
    "    # Zip DataFrame columns of interest: pops\n",
    "    pops = zip(df_pop_ceb['Total Population'],\n",
    "                df_pop_ceb['Urban population (% of total)'])\n",
    "\n",
    "    # Turn zip object into list: pops_list\n",
    "    pops_list = list(pops)\n",
    "\n",
    "    # Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "    df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "    \n",
    "    # Append DataFrame chunk to data: data\n",
    "    data = data.append(df_pop_ceb)\n",
    "\n",
    "# Plot urban population data\n",
    "data.plot(kind='scatter', x='Year', y='Total Urban Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an iterator to load data in chunks (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last leg. You've learned a lot about processing a large dataset in chunks. In this last exercise, you will put all the code for processing the data into a single function so that you can reuse the code without having to rewrite the same things all over again.\n",
    "\n",
    "You're going to define the function plot_pop() which takes two arguments: the filename of the file to be processed, and the country code of the rows you want to process in the dataset.\n",
    "\n",
    "Because all of the previous code you've written in the previous exercises will be housed in plot_pop(), calling the function already does the following:\n",
    "\n",
    "    Loading of the file chunk by chunk,\n",
    "    Creating the new column of urban population values, and\n",
    "    Plotting the urban population data.\n",
    "\n",
    "That's a lot of work, but the function now makes it convenient to repeat the same process for whatever file and country code you want to process and visualize!\n",
    "\n",
    "You're going to use the data from 'ind_pop_data.csv', available in your current directory. The packages pandas and matplotlib.pyplot has been imported as pd and plt respectively for your use.\n",
    "\n",
    "After you are done, take a moment to look at the plots and reflect on the new skills you have acquired. The journey doesn't end here! If you have enjoyed working with this data, you can continue exploring it using the pre-processed version available on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot_pop()\n",
    "def plot_pop(filename, country_code):\n",
    "\n",
    "    # Initialize reader object: urb_pop_reader\n",
    "    urb_pop_reader = pd.read_csv(filename, chunksize=1000)\n",
    "\n",
    "    # Initialize empty DataFrame: data\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each DataFrame chunk\n",
    "    for df_urb_pop in urb_pop_reader:\n",
    "        # Check out specific country: df_pop_ceb\n",
    "        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]\n",
    "\n",
    "        # Zip DataFrame columns of interest: pops\n",
    "        pops = zip(df_pop_ceb['Total Population'],\n",
    "                    df_pop_ceb['Urban population (% of total)'])\n",
    "\n",
    "        # Turn zip object into list: pops_list\n",
    "        pops_list = list(pops)\n",
    "\n",
    "        # Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "    \n",
    "        # Append DataFrame chunk to data: data\n",
    "        data = data.append(df_pop_ceb)\n",
    "\n",
    "    # Plot urban population data\n",
    "    data.plot(kind='scatter', x='Year', y='Total Urban Population')\n",
    "    plt.show()\n",
    "\n",
    "# Set the filename: fn\n",
    "fn = 'ind_pop_data.csv'\n",
    "\n",
    "# Call plot_pop for country code 'CEB'\n",
    "plot_pop('ind_pop_data.csv','CEB')\n",
    "\n",
    "# Call plot_pop for country code 'ARB'\n",
    "plot_pop('ind_pop_data.csv','ARB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
