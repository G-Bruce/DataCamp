{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and viewing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you're going to look at a subset of the Department of Buildings Job Application Filings dataset from the NYC Open Data portal (https://opendata.cityofnewyork.us/). This dataset consists of job applications filed on January 22, 2017.\n",
    "\n",
    "Your first task is to load this dataset into a DataFrame and then inspect it using the .head() and .tail() methods. However, you'll find out very quickly that the printed results don't allow you to see everything you need, since there are too many columns. Therefore, you need to look at the data in another way.\n",
    "\n",
    "The .shape and .columns attributes let you see the shape of the DataFrame and obtain a list of its columns. From here, you can see which columns are relevant to the questions you'd like to ask of the data. To this end, a new DataFrame, df_subset, consisting only of these relevant columns, has been pre-loaded. This is the DataFrame you'll work with in the rest of the chapter.\n",
    "\n",
    "Get acquainted with the dataset now by exploring it with pandas! This initial exploratory analysis is a crucial first step of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Job #  Doc #    Borough House #     Street Name Block Lot    Bin #  \\\n",
      "0  321291945      1   BROOKLYN    1473       44 STREET  5606  49  3136261   \n",
      "1  123370921      1  MANHATTAN      36     E. 3 STREET   458  25  1006549   \n",
      "2  421685439      1     QUEENS   34-56      200 STREET  6078  31  4619372   \n",
      "3  340644128      1   BROOKLYN     354  DOUGLAS STREET   946  15  3019327   \n",
      "4  421677974      2     QUEENS   35-37     36TH STREET   640   4  4009599   \n",
      "\n",
      "  Job Type Job Status    ...     SPECIAL_ACTION_STATUS SPECIAL_ACTION_DATE  \\\n",
      "0       A3          J    ...                         N                 NaN   \n",
      "1       A2          J    ...                         N                 NaN   \n",
      "2       NB          J    ...                         N                 NaN   \n",
      "3       A2          R    ...                         N                 NaN   \n",
      "4       A2          R    ...                         N                 NaN   \n",
      "\n",
      "  BUILDING_CLASS JOB_NO_GOOD_COUNT GIS_LATITUDE GIS_LONGITUDE  \\\n",
      "0             C3                 0    40.635617    -73.985527   \n",
      "1             B9                 0    40.725723    -73.990223   \n",
      "2             A0                 0    40.765658    -73.787240   \n",
      "3             B1                 0    40.679058    -73.981126   \n",
      "4             E9                 0    40.755622    -73.925003   \n",
      "\n",
      "  GIS_COUNCIL_DISTRICT GIS_CENSUS_TRACT           GIS_NTA_NAME    GIS_BIN  \n",
      "0                 44.0            232.0           Borough Park  3136261.0  \n",
      "1                  2.0           3602.0           East Village  1006549.0  \n",
      "2                 19.0           1099.0  Bayside-Bayside Hills        NaN  \n",
      "3                 39.0            131.0     Park Slope-Gowanus  3019327.0  \n",
      "4                 26.0             57.0                Astoria  4009599.0  \n",
      "\n",
      "[5 rows x 96 columns]\n",
      "             Job #  Doc #   Borough House #       Street Name Block Lot  \\\n",
      "1634981  302042858      1  BROOKLYN    2770      OCEAN AVENUE  7404  20   \n",
      "1634982  302042867      1  BROOKLYN     313         76 STREET  5940  78   \n",
      "1634983  302042876      1  BROOKLYN    1658         66 STREET  5559  33   \n",
      "1634984  302042885      1  BROOKLYN     173  LEXINGTON AVENUE  1968  43   \n",
      "1634985  302042894      1  BROOKLYN      52   LOMBARDY STREET  2834  34   \n",
      "\n",
      "           Bin # Job Type Job Status    ...     SPECIAL_ACTION_STATUS  \\\n",
      "1634981  3202124       A3          X    ...                         N   \n",
      "1634982  3148750       A2          R    ...                         N   \n",
      "1634983  3134197       A2          X    ...                         N   \n",
      "1634984  3056361       A2          Q    ...                         N   \n",
      "1634985  3069864       A2          X    ...                         N   \n",
      "\n",
      "        SPECIAL_ACTION_DATE BUILDING_CLASS JOB_NO_GOOD_COUNT GIS_LATITUDE  \\\n",
      "1634981                 NaN            NaN                 0    40.593831   \n",
      "1634982                 NaN            NaN                 0    40.630611   \n",
      "1634983                 NaN            NaN                 0    40.620486   \n",
      "1634984                 NaN            NaN                 0    40.687159   \n",
      "1634985                 NaN            NaN                 0    40.722087   \n",
      "\n",
      "        GIS_LONGITUDE GIS_COUNCIL_DISTRICT GIS_CENSUS_TRACT  \\\n",
      "1634981    -73.950368                 48.0            592.0   \n",
      "1634982    -74.027435                 43.0             66.0   \n",
      "1634983    -73.994067                 38.0            252.0   \n",
      "1634984    -73.955516                 36.0            233.0   \n",
      "1634985    -73.939063                 34.0            449.0   \n",
      "\n",
      "                                           GIS_NTA_NAME    GIS_BIN  \n",
      "1634981  Sheepshead Bay-Gerritsen Beach-Manhattan Beach  3202124.0  \n",
      "1634982                                       Bay Ridge  3148750.0  \n",
      "1634983                                Bensonhurst West  3134197.0  \n",
      "1634984                                         Bedford  3056361.0  \n",
      "1634985                               East Williamsburg  3069864.0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('DOB_Job_Application_Filings.csv')\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Print the tail of df\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: .shape and .columns are attributes, not methods, so you don't need to follow these with parentheses ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1634986, 96)\n",
      "Index([u'Job #', u'Doc #', u'Borough', u'House #', u'Street Name', u'Block',\n",
      "       u'Lot', u'Bin #', u'Job Type', u'Job Status', u'Job Status Descrp',\n",
      "       u'Latest Action Date', u'Building Type', u'Community - Board',\n",
      "       u'Cluster', u'Landmarked', u'Adult Estab', u'Loft Board', u'City Owned',\n",
      "       u'Little e', u'PC Filed', u'eFiling Filed', u'Plumbing', u'Mechanical',\n",
      "       u'Boiler', u'Fuel Burning', u'Fuel Storage', u'Standpipe', u'Sprinkler',\n",
      "       u'Fire Alarm', u'Equipment', u'Fire Suppression', u'Curb Cut', u'Other',\n",
      "       u'Other Description', u'Applicant's First Name',\n",
      "       u'Applicant's Last Name', u'Applicant Professional Title',\n",
      "       u'Applicant License #', u'Professional Cert', u'Pre- Filing Date',\n",
      "       u'Paid', u'Fully Paid', u'Assigned', u'Approved', u'Fully Permitted',\n",
      "       u'Initial Cost', u'Total Est. Fee', u'Fee Status',\n",
      "       u'Existing Zoning Sqft', u'Proposed Zoning Sqft', u'Horizontal Enlrgmt',\n",
      "       u'Vertical Enlrgmt', u'Enlargement SQ Footage', u'Street Frontage',\n",
      "       u'ExistingNo. of Stories', u'Proposed No. of Stories',\n",
      "       u'Existing Height', u'Proposed Height', u'Existing Dwelling Units',\n",
      "       u'Proposed Dwelling Units', u'Existing Occupancy',\n",
      "       u'Proposed Occupancy', u'Site Fill', u'Zoning Dist1', u'Zoning Dist2',\n",
      "       u'Zoning Dist3', u'Special District 1', u'Special District 2',\n",
      "       u'Owner Type', u'Non-Profit', u'Owner's First Name',\n",
      "       u'Owner's Last Name', u'Owner's Business Name', u'Owner's House Number',\n",
      "       u'Owner'sHouse Street Name', u'City ', u'State', u'Zip',\n",
      "       u'Owner'sPhone #', u'Job Description', u'DOBRunDate', u'JOB_S1_NO',\n",
      "       u'TOTAL_CONSTRUCTION_FLOOR_AREA', u'WITHDRAWAL_FLAG', u'SIGNOFF_DATE',\n",
      "       u'SPECIAL_ACTION_STATUS', u'SPECIAL_ACTION_DATE', u'BUILDING_CLASS',\n",
      "       u'JOB_NO_GOOD_COUNT', u'GIS_LATITUDE', u'GIS_LONGITUDE',\n",
      "       u'GIS_COUNCIL_DISTRICT', u'GIS_CENSUS_TRACT', u'GIS_NTA_NAME',\n",
      "       u'GIS_BIN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Print the columns of df\n",
    "print(df.columns)\n",
    "\n",
    "# Print the head and tail of df_subset\n",
    "# print(df_subset.head())\n",
    "# print(df_subset.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, you identified some potentially unclean or missing data. Now, you'll continue to diagnose your data with the very useful .info() method.\n",
    "\n",
    "The .info() method provides important information about a DataFrame, such as the number of rows, number of columns, number of non-missing values in each column, and the data type stored in each column. This is the kind of information that will allow you to confirm whether the 'Initial Cost' and 'Total Est. Fee' columns are numeric or strings. From the results, you'll also be able to see whether or not all columns have complete data in them.\n",
    "\n",
    "The full DataFrame df and the subset DataFrame df_subset have been pre-loaded. Your task is to use the .info() method on these and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1634986 entries, 0 to 1634985\n",
      "Data columns (total 96 columns):\n",
      "Job #                            1634986 non-null int64\n",
      "Doc #                            1634986 non-null int64\n",
      "Borough                          1634986 non-null object\n",
      "House #                          1634986 non-null object\n",
      "Street Name                      1634986 non-null object\n",
      "Block                            1634234 non-null object\n",
      "Lot                              1634230 non-null object\n",
      "Bin #                            1634986 non-null int64\n",
      "Job Type                         1634986 non-null object\n",
      "Job Status                       1634986 non-null object\n",
      "Job Status Descrp                1634986 non-null object\n",
      "Latest Action Date               1634984 non-null object\n",
      "Building Type                    1634986 non-null object\n",
      "Community - Board                1632233 non-null object\n",
      "Cluster                          991643 non-null object\n",
      "Landmarked                       1531670 non-null object\n",
      "Adult Estab                      1455772 non-null object\n",
      "Loft Board                       1261801 non-null object\n",
      "City Owned                       135149 non-null object\n",
      "Little e                         917297 non-null object\n",
      "PC Filed                         602200 non-null object\n",
      "eFiling Filed                    942434 non-null object\n",
      "Plumbing                         539443 non-null object\n",
      "Mechanical                       273795 non-null object\n",
      "Boiler                           48772 non-null object\n",
      "Fuel Burning                     21124 non-null object\n",
      "Fuel Storage                     13969 non-null object\n",
      "Standpipe                        13767 non-null object\n",
      "Sprinkler                        111780 non-null object\n",
      "Fire Alarm                       84872 non-null object\n",
      "Equipment                        301044 non-null object\n",
      "Fire Suppression                 42726 non-null object\n",
      "Curb Cut                         61230 non-null object\n",
      "Other                            930620 non-null object\n",
      "Other Description                929215 non-null object\n",
      "Applicant's First Name           1634927 non-null object\n",
      "Applicant's Last Name            1634974 non-null object\n",
      "Applicant Professional Title     1634903 non-null object\n",
      "Applicant License #              1587230 non-null object\n",
      "Professional Cert                1257145 non-null object\n",
      "Pre- Filing Date                 1634986 non-null object\n",
      "Paid                             1626728 non-null object\n",
      "Fully Paid                       1628918 non-null object\n",
      "Assigned                         1095592 non-null object\n",
      "Approved                         1328877 non-null object\n",
      "Fully Permitted                  1193173 non-null object\n",
      "Initial Cost                     1634986 non-null object\n",
      "Total Est. Fee                   1634986 non-null object\n",
      "Fee Status                       1634986 non-null object\n",
      "Existing Zoning Sqft             1634986 non-null int64\n",
      "Proposed Zoning Sqft             1634986 non-null int64\n",
      "Horizontal Enlrgmt               42313 non-null object\n",
      "Vertical Enlrgmt                 29765 non-null object\n",
      "Enlargement SQ Footage           1634986 non-null int64\n",
      "Street Frontage                  1634986 non-null int64\n",
      "ExistingNo. of Stories           1634986 non-null int64\n",
      "Proposed No. of Stories          1634986 non-null int64\n",
      "Existing Height                  1634986 non-null int64\n",
      "Proposed Height                  1634986 non-null int64\n",
      "Existing Dwelling Units          558891 non-null object\n",
      "Proposed Dwelling Units          833412 non-null object\n",
      "Existing Occupancy               1156270 non-null object\n",
      "Proposed Occupancy               948625 non-null object\n",
      "Site Fill                        1316538 non-null object\n",
      "Zoning Dist1                     1312272 non-null object\n",
      "Zoning Dist2                     178515 non-null object\n",
      "Zoning Dist3                     9395 non-null object\n",
      "Special District 1               234888 non-null object\n",
      "Special District 2               58860 non-null object\n",
      "Owner Type                       1562196 non-null object\n",
      "Non-Profit                       1564029 non-null object\n",
      "Owner's First Name               1634718 non-null object\n",
      "Owner's Last Name                1634779 non-null object\n",
      "Owner's Business Name            1323040 non-null object\n",
      "Owner's House Number             1634779 non-null object\n",
      "Owner'sHouse Street Name         1634970 non-null object\n",
      "City                             1634918 non-null object\n",
      "State                            1634926 non-null object\n",
      "Zip                              1634269 non-null object\n",
      "Owner'sPhone #                   1623494 non-null object\n",
      "Job Description                  1525057 non-null object\n",
      "DOBRunDate                       1634986 non-null object\n",
      "JOB_S1_NO                        1634986 non-null int64\n",
      "TOTAL_CONSTRUCTION_FLOOR_AREA    1634986 non-null int64\n",
      "WITHDRAWAL_FLAG                  1634986 non-null int64\n",
      "SIGNOFF_DATE                     910958 non-null object\n",
      "SPECIAL_ACTION_STATUS            1615212 non-null object\n",
      "SPECIAL_ACTION_DATE              136636 non-null object\n",
      "BUILDING_CLASS                   60728 non-null object\n",
      "JOB_NO_GOOD_COUNT                1634986 non-null int64\n",
      "GIS_LATITUDE                     1628726 non-null float64\n",
      "GIS_LONGITUDE                    1628726 non-null float64\n",
      "GIS_COUNCIL_DISTRICT             1628726 non-null float64\n",
      "GIS_CENSUS_TRACT                 1628726 non-null float64\n",
      "GIS_NTA_NAME                     1628726 non-null object\n",
      "GIS_BIN                          1609963 non-null float64\n",
      "dtypes: float64(5), int64(15), object(76)\n",
      "memory usage: 1.2+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the info of df\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now use the .describe() method to calculate summary statistics of your data.\n",
    "\n",
    "In this exercise, the columns 'Initial Cost' and 'Total Est. Fee' have been cleaned up for you. That is, the dollar sign has been removed and they have been converted into two new numeric columns: initial_cost and total_est_fee. You'll learn how to do this yourself in later chapters. It's also worth noting that some columns such as Job # are encoded as numeric columns, but it does not make sense to compute summary statistics for such columns.\n",
    "\n",
    "This cleaned DataFrame has been pre-loaded as df. Your job is to use the .describe() method on it in the IPython Shell and select the statement below that is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    1.634986e+06\n",
       " mean     8.049458e+01\n",
       " std      2.231575e+02\n",
       " min     -1.200000e+01\n",
       " 25%      0.000000e+00\n",
       " 50%      3.100000e+01\n",
       " 75%      7.100000e+01\n",
       " max      9.999900e+04\n",
       " Name: Proposed Height, dtype: float64, count    1.634986e+06\n",
       " mean     4.866933e+00\n",
       " std      1.411326e+02\n",
       " min      0.000000e+00\n",
       " 25%      0.000000e+00\n",
       " 50%      0.000000e+00\n",
       " 75%      0.000000e+00\n",
       " max      9.855800e+04\n",
       " Name: Street Frontage, dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Proposed Height'].describe(), df['Street Frontage'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency counts for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen, .describe() can only be used on numeric columns. So how can you diagnose data issues when you have categorical data? One way is by using the .value_counts() method, which returns the frequency counts for each unique value in a column!\n",
    "\n",
    "This method also has an optional parameter called dropna which is True by default. What this means is if you have missing data in a column, it will not give a frequency count of them. You want to set the dropna column to False so if there are missing values in a column, it will give you the frequency counts.\n",
    "\n",
    "In this exercise, you're going to look at the 'Borough', 'State', and 'Site Fill' columns to make sure all the values in there are valid. When looking at the output, do a sanity check: Are all values in the 'State' column from NY, for example? Since the dataset consists of applications filed in NY, you would expect this to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANHATTAN        727090\n",
      "BROOKLYN         368709\n",
      "QUEENS           318112\n",
      "BRONX            130282\n",
      "STATEN ISLAND     90793\n",
      "Name: Borough, dtype: int64\n",
      "NY     1588305\n",
      "NJ       25580\n",
      "CT        2626\n",
      "CA        2625\n",
      "FL        2375\n",
      "IL        1868\n",
      "PA        1604\n",
      "MA        1086\n",
      "NC        1066\n",
      "OH        1050\n",
      "VA         981\n",
      "MD         864\n",
      "TX         695\n",
      "GA         469\n",
      "RI         405\n",
      "DC         309\n",
      "NV         292\n",
      "MI         262\n",
      "MN         257\n",
      "TN         235\n",
      "CO         234\n",
      "WA         218\n",
      "AZ         187\n",
      "UT         154\n",
      "SC         146\n",
      "NH         116\n",
      "KS         108\n",
      "MO          95\n",
      "WI          92\n",
      "DE          73\n",
      "NM          68\n",
      "KY          64\n",
      "VT          61\n",
      "NaN         60\n",
      "IN          58\n",
      "IA          45\n",
      "AR          38\n",
      "HI          28\n",
      "ME          23\n",
      "NE          23\n",
      "LA          23\n",
      "OK          18\n",
      "SD          13\n",
      "AK          13\n",
      "AL          12\n",
      "OR          11\n",
      "WY          11\n",
      "ND          10\n",
      "CN           9\n",
      "PR           4\n",
      "ID           3\n",
      "WV           3\n",
      "MS           3\n",
      "ON           3\n",
      "sw           2\n",
      "MT           2\n",
      "FQ           1\n",
      "Name: State, dtype: int64\n",
      "NOT APPLICABLE         620307\n",
      "NONE                   568882\n",
      "NaN                    318448\n",
      "ON-SITE                 74632\n",
      "OFF-SITE                32003\n",
      "USE UNDER 300 CU.YD     20714\n",
      "Name: Site Fill, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the value counts for 'Borough'\n",
    "print(df['Borough'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value_counts for 'State'\n",
    "print(df['State'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for 'Site Fill'\n",
    "print(df['Site Fill'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing single variables with histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, you've been looking at descriptive statistics of your data. One of the best ways to confirm what the numbers are telling you is to plot and visualize the data.\n",
    "\n",
    "You'll start by visualizing single variables using a histogram for numeric values. The column you will work on in this exercise is 'Existing Zoning Sqft'.\n",
    "\n",
    "The .plot() method allows you to create a plot of each column of a DataFrame. The kind parameter allows you to specify the type of plot to use - kind='hist', for example, plots a histogram.\n",
    "\n",
    "In the IPython Shell, begin by computing summary statistics for the 'Existing Zoning Sqft' column using the .describe() method. You'll notice that there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a log scale. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.\n",
    "\n",
    "Finally, note that Python will render a plot such that the axis will hold all the information. That is, if you end up with large amounts of whitespace in your plot, it indicates counts or values too small to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Describe the column\n",
    "df['Existing Zoning Sqft'].describe()\n",
    "\n",
    "# Plot the histogram\n",
    "hist_plt = df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show(hist_plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing multiple variables with boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are great ways of visualizing single variables. To visualize multiple variables, boxplots are useful, especially when one of the variables is categorical.\n",
    "\n",
    "In this exercise, your job is to use a boxplot to compare the 'initial_cost' across the different values of the 'Borough' column. The pandas .boxplot() method is a quick way to do this, in which you have to specify the column and by parameters. Here, you want to visualize how 'initial_cost' varies by 'Borough'.\n",
    "\n",
    "pandas and matplotlib.pyplot have been imported for you as pd and plt, respectively, and the DataFrame has been pre-loaded as df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d7a142114a83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create the boxplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbox_plt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Initial Cost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Borough'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Display the plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36mboxplot_frame\u001b[1;34m(self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds)\u001b[0m\n\u001b[0;32m   2255\u001b[0m     ax = boxplot(self, column=column, by=by, ax=ax, fontsize=fontsize,\n\u001b[0;32m   2256\u001b[0m                  \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2257\u001b[1;33m                  return_type=return_type, **kwds)\n\u001b[0m\u001b[0;32m   2258\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36mboxplot\u001b[1;34m(data, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds)\u001b[0m\n\u001b[0;32m   2224\u001b[0m                                          \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2225\u001b[0m                                          \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2226\u001b[1;33m                                          return_type=return_type)\n\u001b[0m\u001b[0;32m   2227\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36m_grouped_plot_by_column\u001b[1;34m(plotf, data, columns, by, numeric_only, grid, figsize, ax, layout, return_type, **kwargs)\u001b[0m\n\u001b[0;32m   2681\u001b[0m         \u001b[0mgp_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2682\u001b[0m         \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgp_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2683\u001b[1;33m         \u001b[0mre_plotf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplotf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2684\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2685\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpprint_thing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36mplot_group\u001b[1;34m(keys, values, ax)\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpprint_thing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2193\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_na_arraylike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2194\u001b[1;33m         \u001b[0mbp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2196\u001b[0m             \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'both'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\matplotlib\\__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\matplotlib\\axes\\_axes.pyc\u001b[0m in \u001b[0;36mboxplot\u001b[1;34m(self, x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_xticks, autorange, zorder)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m         bxpstats = cbook.boxplot_stats(x, whis=whis, bootstrap=bootstrap,\n\u001b[1;32m-> 3557\u001b[1;33m                                        labels=labels, autorange=autorange)\n\u001b[0m\u001b[0;32m   3558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnotch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m             \u001b[0mnotch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'boxplot.notch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\matplotlib\\cbook\\__init__.pyc\u001b[0m in \u001b[0;36mboxplot_stats\u001b[1;34m(X, whis, bootstrap, labels, autorange)\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[1;31m# arithmetic mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# medians and quartiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   2955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2956\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 2957\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'long'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "box_plt = df.boxplot(column='Initial Cost', by='Borough', rot=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show(box_plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing multiple variables with scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are great when you have a numeric column that you want to compare across different categories. When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "\n",
    "In this exercise, your job is to make a scatter plot with 'initial_cost' on the x-axis and the 'total_est_fee' on the y-axis. You can do this by using the DataFrame .plot() method with kind='scatter'. You'll notice right away that there are 2 major outliers shown in the plots.\n",
    "\n",
    "Since these outliers dominate the plot, an additional DataFrame, df_subset, has been provided, in which some of the extreme values have been removed. After making a scatter plot using this, you'll find some interesting patterns here that would not have been seen by looking at summary statistics or 1 variable plots.\n",
    "\n",
    "When you're done, you can cycle between the two plots by clicking the 'Previous Plot' and 'Next Plot' buttons below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scatter requires x column to be numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9e31d82497e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create and display the first scatter plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scatter'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Initial Cost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'total_est_fee'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   2939\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2941\u001b[1;33m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36mplot_frame\u001b[1;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   1975\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1976\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1977\u001b[1;33m                  **kwds)\n\u001b[0m\u001b[0;32m   1978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1742\u001b[0m             plot_obj = klass(data, x=x, y=y, subplots=subplots, ax=ax,\n\u001b[1;32m-> 1743\u001b[1;33m                              kind=kind, **kwds)\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             raise ValueError(\"plot kind %r can only be used for data frames\"\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, x, y, s, c, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# the handling of this argument later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mScatterPlot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bgranger\\Anaconda2\\lib\\site-packages\\pandas\\plotting\\_core.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_numeric_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' requires x column to be numeric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_numeric_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' requires y column to be numeric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: scatter requires x column to be numeric"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and display the first scatter plot\n",
    "plt.show(df.plot(kind='scatter', x='Initial Cost', y='total_est_fee', rot=70))\n",
    "\n",
    "\n",
    "# Create and display the second scatter plot\n",
    "#plt.show(df_subset.plot(kind='scatter', x='Initial Cost', y='total_est_fee', rot=70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping your data using melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melting data is the process of turning columns of your data into rows of data. Consider the DataFrames from the previous exercise. In the tidy DataFrame, the variables Ozone, Solar.R, Wind, and Temp each had their own column. If, however, you wanted these variables to be in rows instead, you could melt the DataFrame. In doing so, however, you would make the data untidy! This is important to keep in mind: Depending on how your data is represented, you will have to reshape it differently (e.g., this could make it easier to plot values).\n",
    "\n",
    "In this exercise, you will practice melting a DataFrame using pd.melt(). There are two parameters you should be aware of: id_vars and value_vars. The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape), while the value_vars represent the columns you do wish to melt into rows. By default, if no value_vars are provided, all columns not set in the id_vars will be melted. This could save a bit of typing, depending on the number of columns that need to be melted.\n",
    "\n",
    "The (tidy) DataFrame airquality has been pre-loaded. Your job is to melt its Ozone, Solar.R, Wind, and Temp columns into rows. Later in this chapter, you'll learn how to bring this melted DataFrame back into a tidy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(frame=airquality, id_vars=['Month', 'Day'])\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing melted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When melting DataFrames, it would be better to have column names more meaningful than variable and value (the default names used by pd.melt()).\n",
    "\n",
    "The default names may work in certain situations, but it's best to always have data that is self explanatory.\n",
    "\n",
    "You can rename the variable column by specifying an argument to the var_name parameter, and the value column by specifying an argument to the value_name parameter. You will now practice doing exactly this. Pandas as pd and the DataFrame airquality has been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivoting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoting data is the opposite of melting it. Remember the tidy form that the airquality DataFrame was in before you melted it? You'll now begin pivoting it back into that form using the .pivot_table() method!\n",
    "\n",
    "While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.\n",
    "\n",
    ".pivot_table() has an index parameter which you can use to specify the columns that you don't want pivoted: It is similar to the id_vars parameter of pd.melt(). Two other parameters that you have to specify are columns (the name of the column you want to pivot), and values (the values to be used when the column is pivoted). The melted DataFrame airquality_melt has been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "# Pivot airquality_melt: airquality_pivot\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resetting the index of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pivoting airquality_melt in the previous exercise, you didn't quite get back the original DataFrame.\n",
    "\n",
    "What you got back instead was a pandas DataFrame with a hierarchical index (also known as a MultiIndex):\n",
    "https://pandas.pydata.org/pandas-docs/stable/advanced.html\n",
    "\n",
    "Hierarchical indexes are covered in depth in Manipulating DataFrames with pandas:\n",
    "https://www.datacamp.com/courses/manipulating-dataframes-with-pandas. \n",
    "\n",
    "In essence, they allow you to group columns or rows by another variable - in this case, by 'Month' as well as 'Day'.\n",
    "\n",
    "There's a very simple method you can use to get back the original DataFrame from the pivoted DataFrame: .reset_index(). Dan didn't show you how to use this method in the video, but you're now going to practice using it in this exercise to get back the original DataFrame from airquality_pivot, which has been pre-loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot_reset\n",
    "airquality_pivot_reset = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the new index of airquality_pivot_reset\n",
    "print(airquality_pivot_reset.index)\n",
    "\n",
    "# Print the head of airquality_pivot_reset\n",
    "print(airquality_pivot_reset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivoting duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've used the .pivot_table() method when there are multiple index values you want to hold constant during a pivot. In the video, Dan showed you how you can also use pivot tables to deal with duplicate values by providing an aggregation function through the aggfunc parameter. Here, you're going to combine both these uses of pivot tables.\n",
    "\n",
    "Let's say your data collection method accidentally duplicated your dataset. Such a dataset, in which each row is duplicated, has been pre-loaded as airquality_dup. In addition, the airquality_melt DataFrame from the previous exercise has been pre-loaded. Explore their shapes in the IPython Shell by accessing their .shape attributes to confirm the duplicate rows present in airquality_dup.\n",
    "\n",
    "You'll see that by using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index().\n",
    "\n",
    "NumPy and pandas have been imported as np and pd respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table the airquality_dup: airquality_pivot\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Print the head of airquality_pivot before reset_index\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Melt and Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting a column with .str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you saw in the video, consisting of case counts of tuberculosis by country, year, gender, and age group, has been pre-loaded into a DataFrame as tb.\n",
    "\n",
    "In this exercise, you're going to tidy the 'm014' column, which represents males aged 0-14 years of age. In order to parse this value, you need to extract the first letter into a new column for gender, and the rest into a column for age_group. Here, since you can parse values by position, you can take advantage of pandas' vectorized string slicing by using the str attribute of columns of type object.\n",
    "\n",
    "Begin by printing the columns of tb in the IPython Shell using its .columns attribute, and take note of the problematic column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(frame=tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting a column with .split() and .get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way multiple variables are stored in columns is with a delimiter. You'll learn how to deal with such cases in this exercise, using a dataset consisting of Ebola cases and death counts by state and country: https://data.humdata.org/dataset/ebola-cases-2014. It has been pre-loaded into a DataFrame as ebola.\n",
    "\n",
    "Print the columns of ebola in the IPython Shell using ebola.columns. Notice that the data has column names such as Cases_Guinea and Deaths_Guinea. Here, the underscore _ serves as a delimiter between the first part (cases or deaths), and the second part (country).\n",
    "\n",
    "This time, you cannot directly slice the variable by position as in the previous exercise. You now need to use Python's built-in string method called .split(). By default, this method will split a string into parts separated by a space. However, in this case you want it to split by an underscore. You can do this on Cases_Guinea, for example, using Cases_Guinea.split('_'), which returns the list ['Cases', 'Guinea'].\n",
    "\n",
    "The next challenge is to extract the first element of this list and assign it to a type variable, and the second element of the list to a country variable. You can accomplish this by accessing the str attribute of the column and using the .get() method to retrieve the 0 or 1 index, depending on the part you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', \n",
    "value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt['type_country'].str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you'll be working with here relates to NYC Uber data\n",
    "http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014. \n",
    "\n",
    "The original dataset has all the originating Uber pickup locations by time and latitude and longitude. For didactic purposes, you'll be working with a very small portion of the actual data.\n",
    "\n",
    "Three DataFrames have been pre-loaded: uber1, which contains data for April 2014, uber2, which contains data for May 2014, and uber3, which contains data for June 2014. Your job in this exercise is to concatenate these DataFrames together such that the resulting DataFrame has the data for all three months.\n",
    "\n",
    "Begin by exploring the structure of these three DataFrames in the IPython Shell using methods such as .head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining columns of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom. To perform this action, you use the same pd.concat() function, but this time with the keyword argument axis=1. The default, axis=0, is for a row-wise concatenation.\n",
    "\n",
    "You'll return to the Ebola dataset https://data.humdata.org/dataset/ebola-cases-2014\n",
    "you worked with briefly in the last chapter. It has been pre-loaded into a DataFrame called ebola_melt. In this DataFrame, the status and country of a patient is contained in a single column. This column has been parsed into a new DataFrame, status_country, where there are separate columns for status and country.\n",
    "\n",
    "Explore the ebola_melt and status_country DataFrames in the IPython Shell. Your job is to concatenate them column-wise in order to obtain a final, clean DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "# Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom. \n",
    "# To perform this action, you use the same pd.concat() function, but this time with the keyword argument axis=1. The \n",
    "# default, axis=0, is for a row-wise concatenation.\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding and concatenating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding files that match a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to practice using the glob module to find all csv files in the workspace. In the next exercise, you'll programmatically load them into DataFrames.\n",
    "\n",
    "As Dan showed you in the video, the glob module has a function called glob that takes a pattern and returns a list of the files in the working directory that match that pattern.\n",
    "\n",
    "For example, if you know the pattern is part_ single digit number .csv, you can write the pattern as 'part_?.csv' (which would match part_1.csv, part_2.csv, part_3.csv, etc.)\n",
    "\n",
    "Similarly, you can find all .csv files with '*.csv', or all parts with 'part_*'. The ? wildcard represents any 1 character, and the * wildcard represents any number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating and concatenating all matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of filenames to load, you can load all the files into a list of DataFrames that can then be concatenated.\n",
    "\n",
    "You'll start with an empty list called frames. Your job is to use a for loop to:\n",
    "\n",
    "    iterate through each of the filenames\n",
    "    read each filename into a DataFrame, and then\n",
    "    append it to the frames list.\n",
    "\n",
    "You can then concatenate this list of DataFrames using pd.concat(). Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-to-1 data merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.\n",
    "\n",
    "Here, you'll be using survey data that contains readings that William Dyer, Frank Pabodie, and Valentina Roerich took in the late 1920 and 1930 while they were on an expedition towards Antarctica. The dataset was taken from a sqlite database from the Software Carpentry SQL lesson:\n",
    "https://swcarpentry.github.io/sql-novice-survey/\n",
    "\n",
    "Two DataFrames have been pre-loaded: site and visited. Explore them in the IPython Shell and take note of their structure and column names. Your task is to perform a 1-to-1 merge of these two DataFrames using the 'name' column of site and the 'site' column of visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print o2o\n",
    "print(o2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-1 data merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a many-to-one (or one-to-many) merge, one of the values will be duplicated and recycled in the output. That is, one of the keys in the merge is not unique.\n",
    "\n",
    "Here, the two DataFrames site and visited have been pre-loaded once again. Note that this time, visited has multiple entries for the site column. Confirm this by exploring it in the IPython Shell.\n",
    "\n",
    "The .merge() method call is the same as the 1-to-1 merge from the previous exercise, but the data and output will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site,right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(m2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-many data merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final merging scenario occurs when both DataFrames do not have unique keys for a merge. What happens here is that for each duplicated key, every pairwise combination will be created.\n",
    "\n",
    "Two example DataFrames that share common key values have been pre-loaded: df1 and df2. Another DataFrame df3, which is the result of df1 merged with df2, has been pre-loaded. All three DataFrames have been printed - look at the output and notice how pairwise combinations have been created. This example is to help you develop your intuition for many-to-many merges.\n",
    "\n",
    "Here, you'll work with the site and visited DataFrames from before, and a new survey DataFrame. Your task is to merge site and visited as you did in the earlier exercises. You will then merge this merged DataFrame with survey.\n",
    "\n",
    "Begin by exploring the site, visited, and survey DataFrames in the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll see how ensuring all categorical variables in a DataFrame are of type category reduces memory usage.\n",
    "\n",
    "The tips dataset\n",
    "https://github.com/mwaskom/seaborn-data/blob/master/tips.csv\n",
    "has been loaded into a DataFrame called tips. This data contains information about how much a customer tipped, whether the customer was male or female, a smoker or not, etc.\n",
    "\n",
    "Look at the output of tips.info() in the IPython Shell. You'll note that two columns that should be categorical - sex and smoker - are instead of type object, which is pandas' way of storing arbitrary strings. Your job is to convert these two columns to type category and note the reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "\n",
    "# Convert the smoker column to type 'category'\n",
    "tips.smoker = tips.smoker.astype('category')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you expect the data type of a column to be numeric (int or float), but instead it is of type object, this typically means that there is a non numeric value in the column, which also signifies bad data.\n",
    "\n",
    "You can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use the techniques you learned in Chapter 1 to do some exploratory data analysis and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.\n",
    "\n",
    "A modified version of the tips dataset has been pre-loaded into a DataFrame called tips. For instructional purposes, it has been pre-processed to introduce some 'bad' data for you to clean. Use the .info() method to explore this. You'll note that the total_bill and tip columns, which should be numeric, are instead of type object. Your job is to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "\n",
    "# Convert 'tip' to a numeric dtype\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using regular expressions to clean strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String parsing with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video, Dan introduced you to the basics of regular expressions, which are powerful ways of defining patterns to match strings. This exercise will get you started with writing them.\n",
    "\n",
    "When working with data, it is sometimes necessary to write a regular expression to look for properly entered values. Phone numbers in a dataset is a common field that needs to be checked for validity. Your job in this exercise is to define a regular expression to match US phone numbers that fit the pattern of xxx-xxx-xxxx.\n",
    "\n",
    "The regular expression module: https://docs.python.org/3/library/re.html in python is re. When performing pattern matching on data, since the pattern will be used for a match across multiple rows, it's better to compile the pattern first using re.compile(), and then use the compiled pattern to match values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object at 0x0000000005472578>\n",
      "True\n",
      "<_sre.SRE_Match object at 0x0000000005472578>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(result)\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result2 = prog.match('1123-456-7890')\n",
    "print(result)\n",
    "print(bool(result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting numerical values from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting numbers from strings is a common task, particularly when working with unstructured data or log files.\n",
    "\n",
    "Say you have the following string: 'the recipe calls for 6 strawberries and 2 bananas'.\n",
    "\n",
    "It would be useful to extract the 6 and the 2 from this string to be saved for later use when comparing strawberry to banana ratios.\n",
    "\n",
    "When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function. Dan did not discuss this in the video, but it is straightforward to use: You pass in a pattern and a string to re.findall(), and it will return a list of the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '1']\n"
     ]
    }
   ],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll continue practicing your regular expression skills. For each provided string, your job is to write the appropriate pattern to match it.\n",
    "\n",
    "A string of the format: A dollar sign, an arbitrary number of digits, a decimal point, 2 digits.\n",
    "\n",
    "    Use \\$ to match the dollar sign, \\d* to match an arbitrary number of digits, \\. to match the decimal point, and \\d{x} to match x number of digits.\n",
    "\n",
    "A capital letter, followed by an arbitrary number of alphanumeric characters.\n",
    "\n",
    "    Use [A-Z] to match any capital letter followed by \\w* to match an arbitrary number of alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d*\\w*\\d*\\w*\\d*\\w*', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d{3}\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='[A-Z]*', string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using functions to clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions to clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now practice writing functions to clean data.\n",
    "\n",
    "The tips dataset has been pre-loaded into a DataFrame called tips. It has a 'sex' column that contains the values 'Male' or 'Female'. Your job is to write a function that will recode 'Female' to 0, 'Male' to 1, and return np.nan for all entries of 'sex' that are neither 'Female' nor 'Male'.\n",
    "\n",
    "Recoding variables like this is a common data cleaning task. Functions provide a mechanism for you to abstract away complex bits of code as well as reuse code. This makes your code more readable and less error prone.\n",
    "\n",
    "As Dan showed you in the videos, you can use the .apply() method to apply a function across entire rows or columns of DataFrames. However, note that each column of a DataFrame is a pandas Series. Functions can also be applied across Series. Here, you will apply your function over the 'sex' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recode_gender()\n",
    "def recode_gender(gender):\n",
    "\n",
    "    # Return 0 if gender is 'Female'\n",
    "    if gender == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return 1 if gender is 'Male'    \n",
    "    elif gender == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['recode'] = tips.sex.apply(recode_gender)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now be introduced to a powerful Python feature that will help you clean your data more effectively: lambda functions. Instead of using the def syntax that you used in the previous exercise, lambda functions let you make simple, one-line functions.\n",
    "\n",
    "For example, here's a function that squares a variable used in an .apply() method:\n",
    "\n",
    "def my_square(x):\n",
    "    return x ** 2\n",
    "\n",
    "df.apply(my_square)\n",
    "\n",
    "The equivalent code using a lambda function is:\n",
    "\n",
    "df.apply(lambda x: x ** 2)\n",
    "\n",
    "The lambda function takes one parameter - the variable x. The function itself just squares x and returns the result, which is whatever the one line of code evaluates to. In this way, lambda functions can make your code concise and Pythonic.\n",
    "\n",
    "The tips dataset has been pre-loaded into a DataFrame called tips. Your job is to clean its 'total_dollar' column by removing the dollar sign. You'll do this using two different methods: With the .replace() method, and with regular expressions. The regular expression module re has been pre-imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "\n",
    "# Print the head of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate and missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate data causes a variety of problems. From the point of view of performance, they use up unnecessary amounts of memory and cause unneeded calculations to be performed when processing data. In addition, they can also bias any analysis results.\n",
    "\n",
    "A dataset consisting of the performance of songs on the Billboard charts has been pre-loaded into a DataFrame called billboard. Check out its columns in the IPython Shell. Your job in this exercise is to subset this DataFrame and then drop all duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year', 'artist', 'track', 'time']]\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll return to the airquality dataset from Chapter 2. It has been pre-loaded into the DataFrame airquality, and it has missing values for you to practice filling in. Explore airquality in the IPython Shell to checkout which columns have missing values.\n",
    "\n",
    "It's rare to have a (real-world) dataset without any missing values, and it's important to deal with them because certain calculations cannot handle missing values while some calculations will, by default, skip over any missing values.\n",
    "\n",
    "Also, understanding how much missing data you have, and thinking about where it comes from is crucial to making unbiased interpretations of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)\n",
    "\n",
    "# Print the info of airquality\n",
    "print(airquality.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your data with asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll practice writing assert statements using the Ebola dataset from previous chapters to programmatically check for missing values and to confirm that all values are positive. The dataset has been pre-loaded into a DataFrame called ebola.\n",
    "\n",
    "In the video, you saw Dan use the .all() method together with the .notnull() DataFrame method to check for missing values in a column. The .all() method returns True if all values are True. When used on a DataFrame, it returns a Series of Booleans - one for each column in the DataFrame. So if you are using it on a DataFrame, like in this exercise, you need to chain another .all() method so that you return only one True or False value. When using these within an assert statement, nothing will be returned if the assert statement is true: This is how you can confirm that the data you are checking are valid.\n",
    "\n",
    "Note: You can use pd.notnull(df) as an alternative to df.notnull()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that there are no missing values\n",
    "assert ebola.notnull().all().all()\n",
    "\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you obtain a new dataset, your first task should always be to do some exploratory analysis to get a better understanding of the data and diagnose it for any potential issues.\n",
    "\n",
    "The Gapminder data for the 19th century has been loaded into a DataFrame called g1800s. In the IPython Shell, use pandas methods such as .head(), .info(), and .describe(), and DataFrame attributes like .columns and .shape to explore it.\n",
    "\n",
    "Use the information that you acquire from your exploratory analysis to choose the true statement from the options provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 1800, life expectancy around the globe has been steadily going up. You would expect the Gapminder data to confirm this.\n",
    "\n",
    "The DataFrame g1800s has been pre-loaded. Your job in this exercise is to create a scatter plot with life expectancy in '1800' on the x-axis and life expectancy in '1899' on the y-axis.\n",
    "\n",
    "Here, the goal is to visually check the data for insights as well as errors. When looking at the plot, pay attention to whether the scatter plot takes the form of a diagonal line, and which points fall below or above the diagonal line. This will inform how life expectancy in 1899 changed (or did not change) compared to 1800 for different countries. If points fall on a diagonal line, it means that life expectancy remained the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatter plot\n",
    "g1800s.plot(kind='scatter', x='1800', y='1899')\n",
    "\n",
    "# Specify axis labels\n",
    "plt.xlabel('Life Expectancy by Country in 1800')\n",
    "plt.ylabel('Life Expectancy by Country in 1899')\n",
    "\n",
    "# Specify axis limits\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about the question at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are given life expectancy level data by country and year, you could ask questions about how much the average life expectancy changes over each year.\n",
    "\n",
    "Before continuing, however, it's important to make sure that the following assumptions about the data are true:\n",
    "\n",
    "    'Life expectancy' is the first column (index 0) of the DataFrame.\n",
    "    The other columns contain either null or numeric values.\n",
    "    The numeric values are all greater than or equal to 0.\n",
    "    There is only one instance of each country.\n",
    "\n",
    "You can write a function that you can apply over the entire DataFrame to verify some of these assumptions. Note that spending the time to write such a script will help you when working with other datasets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_or_valid(row_data):\n",
    "    \"\"\"Function that takes a row of data,\n",
    "    drops all missing values,\n",
    "    and checks if all remaining values are greater than or equal to 0\n",
    "    \"\"\"\n",
    "    no_na = row_data.dropna()\n",
    "    numeric = pd.to_numeric(no_na)\n",
    "    ge0 = numeric >= 0\n",
    "    return ge0\n",
    "\n",
    "# Check whether the first column is 'Life expectancy'\n",
    "assert g1800s.columns[0] == 'Life expectancy'\n",
    "\n",
    "# Check whether the values in the row (axis=1) are valid\n",
    "assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "\n",
    "# Check that there is only one instance of each country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, three DataFrames have been pre-loaded: g1800s, g1900s, and g2000s. These contain the Gapminder life expectancy data for, respectively, the 19th century, the 20th century, and the 21st century.\n",
    "\n",
    "Your task in this exercise is to concatenate them into a single DataFrame called gapminder. This is a row-wise concatenation, similar to how you concatenated the monthly Uber datasets in Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames row-wise\n",
    "gapminder = pd.concat([g1800s, g1900s, g2000s])\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)\n",
    "\n",
    "# Print the head of gapminder\n",
    "print(gapminder.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial impressions of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have all the data combined into a single DataFrame, the next step is to reshape it into a tidy data format.\n",
    "\n",
    "Currently, the gapminder DataFrame has a separate column for each year. What you want instead is a single column that contains the year, and a single column that represents the average life expectancy for each year and country. By having year in its own column, you can use it as a predictor variable in a later analysis.\n",
    "\n",
    "You can convert the DataFrame into the desired tidy format by melting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Melt gapminder: gapminder_melt\n",
    "gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')\n",
    "\n",
    "# Rename the columns\n",
    "gapminder_melt.columns = ['country', 'year', 'life_expectancy']\n",
    "\n",
    "# Print the head of gapminder_melt\n",
    "print(gapminder_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your data are in the proper shape, you need to ensure that the columns are of the proper data type. That is, you need to ensure that country is of type object, year is of type int64, and life_expectancy is of type float64.\n",
    "\n",
    "The tidy DataFrame has been pre-loaded as gapminder. Explore it in the IPython Shell using the .info() method. Notice that the column 'year' is of type object. This is incorrect, so you'll need to use the pd.to_numeric() function to convert it to a numeric data type.\n",
    "\n",
    "NumPy and pandas have been pre-imported as np and pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder['year'])\n",
    "\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at country spellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having tidied your DataFrame and checked the data types, your next task in the data cleaning process is to look at the 'country' column to see if there are any special or invalid characters you may need to deal with.\n",
    "\n",
    "It is reasonable to assume that country names will contain:\n",
    "\n",
    "    The set of lower and upper case letters.\n",
    "    Whitespace between words.\n",
    "    Periods for any abbreviations.\n",
    "\n",
    "To confirm that this is the case, you can leverage the power of regular expressions again. For common operations like this, Pandas has a built-in string method - str.contains() - which takes a regular expression pattern, and applies it to the Series, returning True if there is a match, and False otherwise.\n",
    "\n",
    "Since here you want to find the values that do not match, you have to invert the boolean, which can be done using ~. This Boolean series can then be used to get the Series of countries that have invalid names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the series of countries: countries\n",
    "countries = gapminder.country\n",
    "\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "\n",
    "# Write the regular expression: pattern\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "\n",
    "# Create the Boolean vector: mask\n",
    "mask = countries.str.contains(pattern)\n",
    "\n",
    "# Invert the mask: mask_inverse\n",
    "mask_inverse = ~mask\n",
    "\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries[mask_inverse]\n",
    "\n",
    "# Print invalid_countries\n",
    "print(invalid_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data cleaning and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to deal with the missing data. There are several strategies for this: You can drop them, fill them in using the mean of the column or row that the missing value is in (also known as imputation https://en.wikipedia.org/wiki/Imputation_(statistics) ), or, if you are dealing with time series data, use a forward fill or backward fill, in which you replace missing values in a column with the most recent known value in the column. See pandas Foundations (https://www.datacamp.com/courses/pandas-foundations) for more on forward fill and backward fill.\n",
    "\n",
    "In general, it is not the best idea to drop missing values, because in doing so you may end up throwing away useful information. In this data, the missing values refer to years where no estimate for life expectancy is available for a given country. You could fill in, or guess what these life expectancies could be by looking at the average life expectancies for other countries in that year, for example. Whichever strategy you go with, it is important to carefully consider all options and understand how they will affect your data.\n",
    "\n",
    "In this exercise, you'll practice dropping missing values. Your job is to drop all the rows that have NaN in the life_expectancy column. Before doing so, it would be valuable to use assert statements to confirm that year and country do not have any missing values.\n",
    "\n",
    "Begin by printing the shape of gapminder in the IPython Shell prior to dropping the missing values. Complete the exercise to find out what its shape will be after dropping the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that country does not contain any missing values\n",
    "assert pd.notnull(gapminder.country).all()\n",
    "\n",
    "# Assert that year does not contain any missing values\n",
    "assert pd.notnull(gapminder.year).all()\n",
    "\n",
    "# Drop the missing values\n",
    "gapminder = gapminder.dropna(axis=0, how='any')\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a clean and tidy dataset, you can do a bit of visualization and aggregation. In this exercise, you'll begin by creating a histogram of the life_expectancy column. You should not get any values under 0 and you should see something reasonable on the higher end of the life_expectancy age range.\n",
    "\n",
    "Your next task is to investigate how average life expectancy changed over the years. To do this, you need to subset the data by each year, get the life_expectancy column from each subset, and take an average of the values. You can achieve this using the .groupby() method:\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html. \n",
    "This .groupby() method is covered in greater depth in Manipulating DataFrames with pandas:\n",
    "https://www.datacamp.com/courses/manipulating-dataframes-with-pandas.\n",
    "\n",
    "Finally, you can save your tidy and summarized DataFrame to a file using the .to_csv() method.\n",
    "\n",
    "matplotlib.pyplot and pandas have been pre-imported as plt and pd. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1) \n",
    "\n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "plt.show()\n",
    "\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "\n",
    "# Print the head of gapminder_agg\n",
    "print(gapminder_agg.head())\n",
    "\n",
    "# Print the tail of gapminder_agg\n",
    "print(gapminder_agg.tail())\n",
    "\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "plt.show()\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save both DataFrames to csv files\n",
    "gapminder.to_csv('gapminder.csv')\n",
    "gapminder_agg.to_csv('gapminder_agg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
